# üöÄ Guide de D√©marrage Rapide

## Pour votre machine locale (avec Ollama d√©j√† install√©)

Vous avez mentionn√© qu'Ollama est d√©j√† install√© sur votre machine locale. Voici comment adapter ce projet :

### 1. Transf√©rer le code sur votre machine locale

```bash
# Sur votre machine locale
git clone <url-du-repo>
cd AgentIA_Commercial
```

### 2. Configuration de l'URL Ollama

√âditez le fichier `.env.local` :

```bash
# Si Ollama tourne sur la m√™me machine
OLLAMA_BASE_URL="http://localhost:11434"

# Si Ollama tourne sur une autre machine de votre r√©seau local
OLLAMA_BASE_URL="http://192.168.X.X:11434"
```

### 3. S'assurer qu'Ollama accepte les connexions

Si vous utilisez Ollama sur une machine distante, lancez-le avec :

```bash
# Sur la machine o√π tourne Ollama
OLLAMA_HOST=0.0.0.0 ollama serve
```

### 4. V√©rifier les mod√®les disponibles

```bash
# Lister les mod√®les install√©s
ollama list

# Si les mod√®les requis ne sont pas install√©s
ollama pull mistral:7b-instruct
ollama pull mixtral:8x7b          # Optionnel mais recommand√©
ollama pull nomic-embed-text      # Pour RAG (Phase 2)
```

### 5. Lancer le setup automatique

```bash
# Script qui configure tout automatiquement
./scripts/dev-setup.sh
```

Ce script va :
- ‚úÖ V√©rifier Node.js et Docker
- ‚úÖ Installer les d√©pendances npm
- ‚úÖ D√©marrer PostgreSQL, ChromaDB et Redis
- ‚úÖ Initialiser la base de donn√©es
- ‚úÖ V√©rifier qu'Ollama est accessible

### 6. D√©marrer l'application

```bash
npm run dev
```

Ouvrez http://localhost:3000 dans votre navigateur !

---

## üìù Configuration Ollama Distant (R√©seau Local)

Si votre Ollama tourne sur une autre machine de votre r√©seau :

### Sur la machine Ollama

```bash
# Permettre les connexions externes
OLLAMA_HOST=0.0.0.0 ollama serve
```

### Sur la machine de d√©veloppement (Next.js)

√âditez `.env.local` :

```bash
# Remplacez par l'IP de votre machine Ollama
OLLAMA_BASE_URL="http://192.168.1.100:11434"
```

Testez la connexion :

```bash
curl http://192.168.1.100:11434/api/tags
```

---

## ‚ö° D√©marrage Ultra-Rapide (tout en une commande)

Si tout est d√©j√† configur√© et que vous voulez juste relancer le projet :

```bash
# D√©marrer Docker + Lancer l'app
docker-compose up -d && npm run dev
```

---

## üêõ Probl√®mes Courants

### "Cannot connect to Ollama"

```bash
# V√©rifier qu'Ollama tourne
curl http://localhost:11434/api/tags

# Si erreur, red√©marrer Ollama
ollama serve
```

### "Database connection failed"

```bash
# V√©rifier Docker
docker-compose ps

# Red√©marrer si n√©cessaire
docker-compose restart postgres
```

### "Module not found @prisma/client"

```bash
# R√©g√©n√©rer Prisma
npm run db:generate
```

---

## üìä Tester les Agents

Une fois l'app lanc√©e, testez chaque type d'agent :

| Prompt | Agent d√©clench√© |
|--------|----------------|
| "Analyse mon pipeline" | SALES_ANALYSIS |
| "R√©dige un email de prospection" | EMAIL_WRITING |
| "Comment am√©liorer mes ventes ?" | COACHING |
| "Bonjour" | GENERAL |

---

## üéØ Prochaines √âtapes

1. ‚úÖ Familiarisez-vous avec l'interface chat
2. ‚úÖ Testez les diff√©rents agents
3. ‚úÖ Explorez le code dans `src/lib/ollama/agents/`
4. üìù Commencez √† personnaliser les prompts dans `src/lib/ollama/prompts/`
5. üóÑÔ∏è Ajoutez vos propres donn√©es CRM dans la DB

---

**Besoin d'aide ?** Consultez le [README complet](./README.md) ou ouvrez une issue !
