# ğŸ¤– AgentIA Commercial

> **Copilot commercial 100% local pour TPE/PME franÃ§aises**

Un assistant IA commercial intelligent qui fonctionne entiÃ¨rement en local avec Ollama. Aucune dÃ©pendance aux API cloud, vos donnÃ©es restent privÃ©es et sÃ©curisÃ©es.

## ğŸ¯ FonctionnalitÃ©s (Phase 1 - MVP)

- âœ… **Chat intelligent** avec routing automatique vers des agents spÃ©cialisÃ©s
- âœ… **Analyse commerciale** : Insights sur votre pipeline et vos performances
- âœ… **RÃ©daction d'emails** : GÃ©nÃ©ration d'emails commerciaux personnalisÃ©s
- âœ… **CRM intÃ©grÃ©** : SchÃ©ma complet pour gÃ©rer contacts, deals et activitÃ©s
- âœ… **100% Local** : PropulsÃ© par Ollama (Mistral, Mixtral, Llama 3.1)
- âœ… **Architecture multi-agents** : Router, Sales Analyst, Email Writer, Coach

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Next.js 14 (App Router)        â”‚
â”‚  - Interface Chat                   â”‚
â”‚  - API Routes                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Agents Ollama (LangChain)       â”‚
â”‚  - Router Agent                     â”‚
â”‚  - Sales Analyst Agent              â”‚
â”‚  - Email Writer Agent               â”‚
â”‚  - General Agent                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Ollama (Local LLM)             â”‚
â”‚  - Mistral 7B                       â”‚
â”‚  - Mixtral 8x7B                     â”‚
â”‚  - Llama 3.1 8B                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ PrÃ©requis

### Obligatoire

- **Node.js** 20+ et npm 10+
- **Docker** et Docker Compose (pour PostgreSQL, ChromaDB, Redis)
- **Ollama** installÃ© et lancÃ© localement

### Optionnel

- **GPU** : RecommandÃ© pour de meilleures performances (Mixtral 8x7B)
- **RAM** : Minimum 16 GB, recommandÃ© 32 GB

## ğŸš€ Installation

### 1. Cloner le projet

```bash
git clone <url-du-repo>
cd AgentIA_Commercial
```

### 2. Installer Ollama et les modÃ¨les

**Sur Linux/Mac** :

```bash
# Installer Ollama (si pas dÃ©jÃ  fait)
curl -fsSL https://ollama.com/install.sh | sh

# DÃ©marrer Ollama
ollama serve

# Dans un autre terminal, tÃ©lÃ©charger les modÃ¨les
ollama pull mistral:7b-instruct
ollama pull mixtral:8x7b          # Optionnel mais recommandÃ©
ollama pull llama3.1:8b           # Optionnel
ollama pull nomic-embed-text      # Pour le RAG (Phase 2)
```

**Sur Windows** :

TÃ©lÃ©chargez et installez depuis [https://ollama.com/download](https://ollama.com/download)

**Note** : Si votre Ollama est sur une machine distante, modifiez `OLLAMA_BASE_URL` dans `.env.local`

### 3. Installer les dÃ©pendances npm

```bash
npm install
```

### 4. Configurer les variables d'environnement

Le fichier `.env.local` a dÃ©jÃ  Ã©tÃ© crÃ©Ã©. **Modifiez-le selon votre configuration** :

```bash
# Si Ollama est sur une autre machine
OLLAMA_BASE_URL="http://192.168.1.100:11434"

# Si vous voulez utiliser des modÃ¨les diffÃ©rents
OLLAMA_ANALYST_MODEL="llama3.1:70b"
```

### 5. DÃ©marrer les services Docker

```bash
# DÃ©marrer PostgreSQL, ChromaDB et Redis
docker-compose up -d

# VÃ©rifier que tout fonctionne
docker-compose ps
```

Vous devriez voir 3 conteneurs en cours d'exÃ©cution :
- `agentia_postgres`
- `agentia_chromadb`
- `agentia_redis`

### 6. Initialiser la base de donnÃ©es

```bash
# GÃ©nÃ©rer le client Prisma
npm run db:generate

# CrÃ©er les tables
npm run db:push
```

### 7. Lancer l'application

```bash
npm run dev
```

L'application est accessible sur **http://localhost:3000**

## ğŸ§ª Tester l'application

Une fois lancÃ©e, testez les diffÃ©rents agents :

### Test 1 : Analyse commerciale
```
Prompt : "Analyse mon pipeline actuel et dis-moi sur quoi me concentrer"
Agent : SALES_ANALYSIS
```

### Test 2 : RÃ©daction d'email
```
Prompt : "RÃ©dige un email de prospection pour un prospect dans le secteur tech"
Agent : EMAIL_WRITING
```

### Test 3 : Conseil commercial
```
Prompt : "Comment puis-je amÃ©liorer mon taux de conversion ?"
Agent : COACHING
```

### Test 4 : Conversation gÃ©nÃ©rale
```
Prompt : "Bonjour, qu'est-ce que tu peux faire pour moi ?"
Agent : GENERAL
```

## ğŸ“ Structure du projet

```
AgentIA_Commercial/
â”œâ”€â”€ prisma/
â”‚   â”œâ”€â”€ schema.prisma          # SchÃ©ma CRM complet
â”‚   â””â”€â”€ migrations/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/chat/
â”‚   â”‚   â”‚   â””â”€â”€ route.ts       # API route principale
â”‚   â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”‚   â”œâ”€â”€ page.tsx
â”‚   â”‚   â””â”€â”€ globals.css
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”‚   â””â”€â”€ chat-interface.tsx
â”‚   â”‚   â””â”€â”€ ui/                # Composants Shadcn UI
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ ollama/
â”‚   â”‚   â”‚   â”œâ”€â”€ client.ts      # Client Ollama
â”‚   â”‚   â”‚   â”œâ”€â”€ agents/        # Agents spÃ©cialisÃ©s
â”‚   â”‚   â”‚   â””â”€â”€ prompts/       # System prompts
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â””â”€â”€ prisma.ts
â”‚   â”‚   â””â”€â”€ utils.ts
â”‚   â””â”€â”€ types/
â”‚       â””â”€â”€ index.ts           # Types TypeScript
â”œâ”€â”€ docker-compose.yml         # PostgreSQL + ChromaDB + Redis
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

## ğŸ”§ Configuration avancÃ©e

### Changer les modÃ¨les Ollama

Ã‰ditez `.env.local` :

```bash
# Utiliser Llama 3.1 70B pour l'analyse (nÃ©cessite beaucoup de RAM)
OLLAMA_ANALYST_MODEL="llama3.1:70b"

# Utiliser Qwen2.5 pour la rÃ©daction
OLLAMA_WRITER_MODEL="qwen2.5:7b"
```

Puis redÃ©marrez le serveur Next.js.

### Ajuster la tempÃ©rature des LLM

Dans `src/lib/ollama/agents/*.ts`, modifiez le paramÃ¨tre `temperature` :

```typescript
private llm = createOllamaClient(model, {
  temperature: 0.9, // Plus crÃ©atif
})
```

### Utiliser Ollama distant

Si Ollama tourne sur un serveur distant :

```bash
# Dans .env.local
OLLAMA_BASE_URL="http://192.168.1.100:11434"
```

Assurez-vous qu'Ollama accepte les connexions externes :

```bash
OLLAMA_HOST=0.0.0.0 ollama serve
```

## ğŸ³ Commandes Docker utiles

```bash
# DÃ©marrer les services
docker-compose up -d

# ArrÃªter les services
docker-compose down

# Voir les logs
docker-compose logs -f

# Supprimer les donnÃ©es (âš ï¸ supprime la DB)
docker-compose down -v
```

## ğŸ—„ï¸ Commandes Prisma

```bash
# GÃ©nÃ©rer le client Prisma
npm run db:generate

# CrÃ©er/appliquer les migrations
npm run db:migrate

# Push le schÃ©ma sans migration
npm run db:push

# Ouvrir Prisma Studio (interface graphique)
npm run db:studio
```

## ğŸ› ï¸ DÃ©veloppement

### Scripts npm disponibles

```bash
npm run dev          # Lancer en mode dÃ©veloppement
npm run build        # Build pour production
npm run start        # Lancer en production
npm run lint         # Linter ESLint
npm run setup        # Setup complet (Docker + DB)
```

### Ajouter un nouvel agent

1. CrÃ©er le fichier dans `src/lib/ollama/agents/mon-agent.ts`
2. Ajouter le prompt dans `src/lib/ollama/prompts/system-prompts.ts`
3. Mettre Ã  jour le router dans `src/lib/ollama/agents/router.ts`
4. Ajouter le cas dans `src/app/api/chat/route.ts`

## ğŸ“Š DonnÃ©es mockÃ©es (Phase 1)

Pour tester l'agent Sales Analyst, des donnÃ©es mockÃ©es sont utilisÃ©es dans `src/app/api/chat/route.ts`.

**Phase 2** intÃ©grera les vraies donnÃ©es depuis la base PostgreSQL.

## ğŸ”’ SÃ©curitÃ© et RGPD

- âœ… **100% local** : Aucune donnÃ©e n'est envoyÃ©e Ã  des services tiers
- âœ… **DonnÃ©es chiffrÃ©es** : PostgreSQL peut Ãªtre configurÃ© avec chiffrement
- âœ… **Pas de tÃ©lÃ©mÃ©trie** : ChromaDB configurÃ© sans tÃ©lÃ©mÃ©trie
- âœ… **RGPD-friendly** : Toutes les donnÃ©es restent sous votre contrÃ´le

## ğŸš§ Roadmap

### Phase 1 (âœ… Actuelle - MVP)
- âœ… Architecture de base
- âœ… Chat intelligent avec Ollama
- âœ… Agents spÃ©cialisÃ©s (Router, Analyst, Writer, General)
- âœ… SchÃ©ma CRM complet
- âœ… Interface utilisateur moderne

### Phase 2 (ğŸš€ Prochaine)
- ğŸ“‹ IntÃ©gration CRM complÃ¨te (CRUD contacts/deals)
- ğŸ§  RAG avec ChromaDB (mÃ©moire long-terme)
- ğŸ“Š Dashboard analytics
- ğŸ“§ Interface de gestion d'emails
- âœ… Agent CRM Assistant
- âœ… Agent Coach

### Phase 3
- ğŸ¤– Automatisations (scoring leads, alertes)
- ğŸ“ˆ PrÃ©visions IA
- ğŸ“± Version mobile
- ğŸ”— IntÃ©grations (Gmail, calendrier, etc.)

## ğŸ› Troubleshooting

### Ollama ne rÃ©pond pas

```bash
# VÃ©rifier qu'Ollama tourne
curl http://localhost:11434/api/tags

# RedÃ©marrer Ollama
killall ollama
ollama serve
```

### Erreur de connexion PostgreSQL

```bash
# VÃ©rifier que Docker tourne
docker-compose ps

# Voir les logs PostgreSQL
docker-compose logs postgres

# RedÃ©marrer PostgreSQL
docker-compose restart postgres
```

### Les modÃ¨les Ollama sont lents

- Utilisez des modÃ¨les plus petits (`mistral:7b` au lieu de `mixtral:8x7b`)
- Activez le GPU si disponible
- Augmentez la RAM allouÃ©e Ã  Docker

### Erreur "Module not found"

```bash
# RÃ©installer les dÃ©pendances
rm -rf node_modules package-lock.json
npm install

# RÃ©gÃ©nÃ©rer Prisma
npm run db:generate
```

## ğŸ“ Licence

Ce projet est sous licence MIT. Libre Ã  vous de l'utiliser, le modifier et le distribuer.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir des issues ou des pull requests.

## ğŸ“§ Support

Pour toute question ou problÃ¨me, ouvrez une issue sur GitHub.

---

**Fait avec â¤ï¸ pour les entrepreneurs franÃ§ais qui veulent garder le contrÃ´le de leurs donnÃ©es**
